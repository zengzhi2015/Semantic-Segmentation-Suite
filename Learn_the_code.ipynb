{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check all functions in helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import itertools\n",
    "import operator\n",
    "import os, csv\n",
    "import tensorflow as tf\n",
    "\n",
    "import time, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. [class_names, label_values] = get_label_info(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_info(csv_path):\n",
    "    \"\"\"\n",
    "    Retrieve the class names and label values for the selected dataset.\n",
    "    Must be in CSV format!\n",
    "\n",
    "    # Arguments\n",
    "        csv_path: The file path of the class dictionairy\n",
    "        \n",
    "    # Returns\n",
    "        Two lists: one for the class names and the other for the label values\n",
    "    \"\"\"\n",
    "    filename, file_extension = os.path.splitext(csv_path)\n",
    "    if not file_extension == \".csv\":\n",
    "        return ValueError(\"File is not a CSV!\")\n",
    "\n",
    "    class_names = []\n",
    "    label_values = []\n",
    "    with open(csv_path, 'r') as csvfile:\n",
    "        file_reader = csv.reader(csvfile, delimiter=',')\n",
    "        header = next(file_reader)\n",
    "        for row in file_reader:\n",
    "            class_names.append(row[0])\n",
    "            label_values.append([int(row[1]), int(row[2]), int(row[3])])\n",
    "        # print(class_dict)\n",
    "    return class_names, label_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"E:\\\\Python Workspace\\\\Semantic-Segmentation-Suite\\\\CamVid\\\\class_dict.csv\"\n",
    "class_names, label_values = get_label_info(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal [64, 128, 64]\n",
      "Archway [192, 0, 128]\n",
      "Bicyclist [0, 128, 192]\n",
      "Bridge [0, 128, 64]\n",
      "Building [128, 0, 0]\n",
      "Car [64, 0, 128]\n",
      "CartLuggagePram [64, 0, 192]\n",
      "Child [192, 128, 64]\n",
      "Column_Pole [192, 192, 128]\n",
      "Fence [64, 64, 128]\n",
      "LaneMkgsDriv [128, 0, 192]\n",
      "LaneMkgsNonDriv [192, 0, 64]\n",
      "Misc_Text [128, 128, 64]\n",
      "MotorcycleScooter [192, 0, 192]\n",
      "OtherMoving [128, 64, 64]\n",
      "ParkingBlock [64, 192, 128]\n",
      "Pedestrian [64, 64, 0]\n",
      "Road [128, 64, 128]\n",
      "RoadShoulder [128, 128, 192]\n",
      "Sidewalk [0, 0, 192]\n",
      "SignSymbol [192, 128, 128]\n",
      "Sky [128, 128, 128]\n",
      "SUVPickupTruck [64, 128, 192]\n",
      "TrafficCone [0, 0, 64]\n",
      "TrafficLight [0, 64, 64]\n",
      "Train [192, 64, 128]\n",
      "Tree [128, 128, 0]\n",
      "Truck_Bus [192, 128, 192]\n",
      "Tunnel [64, 0, 64]\n",
      "VegetationMisc [192, 192, 0]\n",
      "Void [0, 0, 0]\n",
      "Wall [64, 192, 0]\n"
     ]
    }
   ],
   "source": [
    "for name, value in zip(class_names, label_values):\n",
    "    print(name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. semantic_map = one_hot_it(label, label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_it(label, label_values):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    # st = time.time()\n",
    "    # w = label.shape[0]\n",
    "    # h = label.shape[1]\n",
    "    # num_classes = len(class_dict)\n",
    "    # x = np.zeros([w,h,num_classes])\n",
    "    # unique_labels = sortedlist((class_dict.values()))\n",
    "    # for i in range(0, w):\n",
    "    #     for j in range(0, h):\n",
    "    #         index = unique_labels.index(list(label[i][j][:]))\n",
    "    #         x[i,j,index]=1\n",
    "    # print(\"Time 1 = \", time.time() - st)\n",
    "\n",
    "    # st = time.time()\n",
    "    # https://stackoverflow.com/questions/46903885/map-rgb-semantic-maps-to-one-hot-encodings-and-vice-versa-in-tensorflow\n",
    "    # https://stackoverflow.com/questions/14859458/how-to-check-if-all-values-in-the-columns-of-a-numpy-matrix-are-the-same\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        # colour_map = np.full((label.shape[0], label.shape[1], label.shape[2]), colour, dtype=int)\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "    # print(\"Time 2 = \", time.time() - st)\n",
    "\n",
    "    return semantic_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = np.array([[0,0,1],[2,0,1],[0,3,1]])\n",
    "#label_values = np.array([0,1,2,3])\n",
    "#semantic_map = one_hot_it(label, label_values)\n",
    "label = np.array([[[0,0,0],[0,0,0],[0,0,1]],[[0,1,0],[0,0,0],[0,0,1]],[[0,0,0],[0,1,1],[0,0,1]]])\n",
    "label_values = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1]])\n",
    "semantic_map = one_hot_it(label, label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3)\n",
      "(3, 3, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "print(label.shape)\n",
    "print(semantic_map.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original function does not work properly. I have to correct it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_it(label, label_values):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 3D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    semantic_map = np.zeros([label.shape[0],label.shape[1],label_values.shape[0]])\n",
    "    for i_colour in range(label_values.shape[0]):\n",
    "        colour = label_values[i_colour]\n",
    "        equality = np.zeros_like(label,dtype=bool)\n",
    "        for i_channel in range(colour.shape[0]):\n",
    "            equality[:,:,i_channel] = np.equal(label[:,:,i_channel], colour[i_channel])\n",
    "        semantic_map[:,:,i_colour] = np.all(equality, axis = -1)\n",
    "        \n",
    "    semantic_map = semantic_map.astype(int)\n",
    "\n",
    "    return semantic_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colour.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = np.array([[0,0,3],[6,0,3],[0,9,3]])\n",
    "#label_values = np.array([0,3,6,9])\n",
    "label = np.array([[[0,0,0],[0,0,0],[0,0,1]],[[0,1,0],[0,0,0],[0,0,1]],[[0,0,0],[0,1,1],[0,0,1]]])\n",
    "label_values = np.array([[0,0,0],[0,0,1],[0,1,0],[0,1,1]])\n",
    "semantic_map = one_hot_it(label, label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: \n",
      " [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 1]]\n",
      "\n",
      " [[0 1 0]\n",
      "  [0 0 0]\n",
      "  [0 0 1]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 1 1]\n",
      "  [0 0 1]]]\n",
      "semantic_map: \n",
      " [[[1 0 0 0]\n",
      "  [1 0 0 0]\n",
      "  [0 1 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [1 0 0 0]\n",
      "  [0 1 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 1 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "print('label: \\n',label)\n",
    "print('semantic_map: \\n',semantic_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. x = reverse_one_hot(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_one_hot(semantic_map):\n",
    "    \"\"\"\n",
    "    Transform a 3D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    x = np.argmax(semantic_map, axis = -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_map = reverse_one_hot(semantic_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [2 0 1]\n",
      " [0 3 1]]\n"
     ]
    }
   ],
   "source": [
    "print(key_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. x = colour_code_segmentation(image, label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colour_code_segmentation(key_map, label_values):\n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[key_map.astype(int)]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = colour_code_segmentation(key_map, label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 3]\n",
      " [6 0 3]\n",
      " [0 9 3]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check all functions in utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os,time,cv2, sys, math\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import os, random\n",
    "from scipy.misc import imread\n",
    "import ast\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. file_name = filepath_to_name(full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes an absolute file path and returns the name of the file without th extension\n",
    "def filepath_to_name(full_name):\n",
    "    file_name = os.path.basename(full_name)\n",
    "    file_name = os.path.splitext(file_name)[0]\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Python Workspace\\Semantic-Segmentation-Suite\\CamVid\\class_dict.csv\n",
      "Base Name:  class_dict.csv\n",
      "('class_dict', '.csv')\n",
      "class_dict\n"
     ]
    }
   ],
   "source": [
    "full_name = \"E:\\\\Python Workspace\\\\Semantic-Segmentation-Suite\\\\CamVid\\\\class_dict.csv\"\n",
    "print(full_name)\n",
    "print('Base Name: ',os.path.basename(full_name))\n",
    "print(os.path.splitext(os.path.basename(full_name)))\n",
    "print(filepath_to_name(full_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. LOG(X, f=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print with time. To console or file\n",
    "def LOG(X, f=None):\n",
    "    time_stamp = datetime.datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "    if not f:\n",
    "        print(time_stamp + \" \" + X)\n",
    "    else:\n",
    "        f.write(time_stamp + \" \" + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-05-04 21:05:02] test information\n"
     ]
    }
   ],
   "source": [
    "X = 'test information'\n",
    "LOG(X, f=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total number of trainable parameters in the model\n",
    "def count_params():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        total_parameters += variable_parameters\n",
    "    print(\"This model has %d trainable parameters\"% (total_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. output = mean_image_subtraction(inputs, means=[123.68, 116.78, 103.94])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtracts the mean images from ImageNet\n",
    "def mean_image_subtraction(inputs, means=[123.68, 116.78, 103.94]):\n",
    "    inputs=tf.to_float(inputs)\n",
    "    # Get the last dim of inputs\n",
    "    num_channels = inputs.get_shape().as_list()[-1]\n",
    "    if len(means) != num_channels:\n",
    "        raise ValueError('len(means) must match the number of channels')\n",
    "    # split the image to independent channels\n",
    "    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=inputs)\n",
    "    for i in range(num_channels):\n",
    "        channels[i] -= means[i]\n",
    "    return tf.concat(axis=3, values=channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  0. 100. 200.]\n",
      "   [ 25. 125. 225.]]\n",
      "\n",
      "  [[ 50. 150. 250.]\n",
      "   [ 75. 175. 275.]]]]\n",
      "[1, 2, 2, 3]\n",
      "3\n",
      "3\n",
      "[[[[ 0.]\n",
      "   [25.]]\n",
      "\n",
      "  [[50.]\n",
      "   [75.]]]]\n",
      "[[[[100.]\n",
      "   [125.]]\n",
      "\n",
      "  [[150.]\n",
      "   [175.]]]]\n",
      "[[[[200.]\n",
      "   [225.]]\n",
      "\n",
      "  [[250.]\n",
      "   [275.]]]]\n",
      "[[[[-123.68      -16.779999   96.06    ]\n",
      "   [ -98.68        8.220001  121.06    ]]\n",
      "\n",
      "  [[ -73.68       33.22      146.06    ]\n",
      "   [ -48.68       58.22      171.06    ]]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.get_default_graph()\n",
    "sess = tf.Session(graph=graph)\n",
    "\n",
    "inputs = tf.constant([[[[0,100,200],[25,125,225]],[[50,150,250],[75,175,275]]],])\n",
    "# inputs = tf.constant([[[[0,100],[25,125]],[[50,150],[75,175]]],])\n",
    "inputs=tf.to_float(inputs)\n",
    "print(sess.run(inputs))\n",
    "num_channels = inputs.get_shape().as_list()[-1]\n",
    "print(inputs.get_shape().as_list())\n",
    "print(num_channels)\n",
    "means=[123.68, 116.78, 103.94]\n",
    "print(len(means))\n",
    "channels = tf.split(axis=3, num_or_size_splits=num_channels, value=inputs)\n",
    "print(sess.run(channels[0]))\n",
    "print(sess.run(channels[1]))\n",
    "print(sess.run(channels[2]))\n",
    "print(sess.run(mean_image_subtraction(inputs, means=[123.68, 116.78, 103.94])))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 cropped_image = random_crop(image, label, crop_height, crop_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly crop the image to a specific size. For data augmentation\n",
    "def random_crop(image, label, crop_height, crop_width):\n",
    "    if (image.shape[0] != label.shape[0]) or (image.shape[1] != label.shape[1]):\n",
    "        raise Exception('Image and label must have the same dimensions!')\n",
    "        \n",
    "    if (crop_width <= image.shape[1]) and (crop_height <= image.shape[0]):\n",
    "        x = random.randint(0, image.shape[1]-crop_width)\n",
    "        y = random.randint(0, image.shape[0]-crop_height)\n",
    "        \n",
    "        if len(label.shape) == 3:\n",
    "            return image[y:y+crop_height, x:x+crop_width, :], label[y:y+crop_height, x:x+crop_width, :]\n",
    "        else:\n",
    "            return image[y:y+crop_height, x:x+crop_width, :], label[y:y+crop_height, x:x+crop_width]\n",
    "    else:\n",
    "        raise Exception('Crop shape exceeds image dimensions!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function applies only for images not for tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a32acdc30be5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcropped_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_crop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcropped_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-a78c90a419b6>\u001b[0m in \u001b[0;36mrandom_crop\u001b[1;34m(image, label, crop_height, crop_width)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcrop_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcrop_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcrop_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcrop_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcrop_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcrop_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcrop_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcrop_width\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Crop shape exceeds image dimensions!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "image = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "label = np.array([[1,1,0],[1,0,1],[0,1,1]])\n",
    "cropped_image = random_crop(image, label, 2, 2)\n",
    "print(cropped_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function has problem. I rewrite the code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly crop the image to a specific size. For data augmentation\n",
    "def random_crop(image, label, crop_height, crop_width):\n",
    "    if (image.shape[0] != label.shape[0]) or (image.shape[1] != label.shape[1]):\n",
    "        raise Exception('Image and label must have the same dimensions!')\n",
    "        \n",
    "    if (crop_width <= image.shape[1]) and (crop_height <= image.shape[0]):\n",
    "        x = random.randint(0, image.shape[1]-crop_width)\n",
    "        y = random.randint(0, image.shape[0]-crop_height)\n",
    "        \n",
    "        if len(label.shape) == 3:\n",
    "            return image[y:y+crop_height, x:x+crop_width, :], label[y:y+crop_height, x:x+crop_width, :]\n",
    "        else:\n",
    "            return image[y:y+crop_height, x:x+crop_width], label[y:y+crop_height, x:x+crop_width]\n",
    "    else:\n",
    "        raise Exception('Crop shape exceeds image dimensions!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[1 1 0]\n",
      " [1 0 1]\n",
      " [0 1 1]]\n",
      "(array([[2, 3],\n",
      "       [5, 6]]), array([[1, 0],\n",
      "       [0, 1]]))\n"
     ]
    }
   ],
   "source": [
    "image = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "label = np.array([[1,1,0],[1,0,1],[0,1,1]])\n",
    "cropped_image = random_crop(image, label, 2, 2)\n",
    "print(image)\n",
    "print(label)\n",
    "print(cropped_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 global_accuracy = compute_global_accuracy(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average segmentation accuracy across all classes\n",
    "def compute_global_accuracy(pred, label):\n",
    "    total = len(label)\n",
    "    count = 0.0\n",
    "    for i in range(total):\n",
    "        if pred[i] == label[i]:\n",
    "            count = count + 1.0\n",
    "    return float(count) / float(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This definition only applies to 1d vector. So, I modified this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average segmentation accuracy across all classes\n",
    "def compute_global_accuracy(pred, label):\n",
    "    f_pred = pred.flatten()\n",
    "    f_label = label.flatten()\n",
    "    count = np.sum(f_pred==f_label)\n",
    "    return float(count) / float(len(f_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "pred = np.array([[1,0,1],[1,0,1],[0,1,1]])\n",
    "label = np.array([[0,1,0],[1,0,1],[0,1,1]])\n",
    "global_accuracy = compute_global_accuracy(pred, label)\n",
    "print(global_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 accuracies = compute_class_accuracies(pred, label, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the class-specific segmentation accuracy\n",
    "def compute_class_accuracies(pred, label, num_classes):\n",
    "    total = []\n",
    "    for val in range(num_classes):\n",
    "        total.append((label == val).sum())\n",
    "\n",
    "    count = [0.0] * num_classes\n",
    "    for i in range(len(label)):\n",
    "        if pred[i] == label[i]:\n",
    "            count[int(pred[i])] = count[int(pred[i])] + 1.0\n",
    "\n",
    "    # If there are no pixels from a certain class in the GT, \n",
    "    # it returns NAN because of divide by zero\n",
    "    # Replace the nans with a 1.0.\n",
    "    accuracies = []\n",
    "    for i in range(len(total)):\n",
    "        if total[i] == 0:\n",
    "            accuracies.append(1.0)\n",
    "        else:\n",
    "            accuracies.append(count[i] / total[i])\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This definition only applies to 1d vector. So, I modified this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the class-specific segmentation accuracy\n",
    "def compute_class_accuracies(pred, label, num_classes):\n",
    "    pred = pred.flatten()\n",
    "    label = label.flatten()\n",
    "    total = []\n",
    "    for val in range(num_classes):\n",
    "        total.append((label == val).sum())\n",
    "\n",
    "    count = [0.0] * num_classes\n",
    "    for i in range(len(label)):\n",
    "        if pred[i] == label[i]:\n",
    "            count[int(pred[i])] = count[int(pred[i])] + 1.0\n",
    "\n",
    "    # If there are no pixels from a certain class in the GT, \n",
    "    # it returns NAN because of divide by zero\n",
    "    # Replace the nans with a 1.0.\n",
    "    accuracies = []\n",
    "    for i in range(len(total)):\n",
    "        if total[i] == 0:\n",
    "            accuracies.append(1.0)\n",
    "        else:\n",
    "            accuracies.append(count[i] / total[i])\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.8]\n"
     ]
    }
   ],
   "source": [
    "print(compute_class_accuracies(pred, label, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 mean_iou = compute_mean_iou(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_iou(pred, label):\n",
    "\n",
    "    unique_labels = np.unique(label)\n",
    "    num_unique_labels = len(unique_labels);\n",
    "\n",
    "    I = np.zeros(num_unique_labels)\n",
    "    U = np.zeros(num_unique_labels)\n",
    "\n",
    "    for index, val in enumerate(unique_labels):\n",
    "        pred_i = pred == val\n",
    "        label_i = label == val\n",
    "\n",
    "        I[index] = float(np.sum(np.logical_and(label_i, pred_i)))\n",
    "        U[index] = float(np.sum(np.logical_or(label_i, pred_i)))\n",
    "\n",
    "\n",
    "    mean_iou = np.mean(I / U)\n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This definition only applies to 1d vector. So, I modified this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_iou(pred, label):\n",
    "    pred = pred.flatten()\n",
    "    label = label.flatten()\n",
    "    \n",
    "    unique_labels = np.unique(label)\n",
    "    num_unique_labels = len(unique_labels);\n",
    "\n",
    "    I = np.zeros(num_unique_labels)\n",
    "    U = np.zeros(num_unique_labels)\n",
    "\n",
    "    for index, val in enumerate(unique_labels):\n",
    "        pred_i = pred == val\n",
    "        label_i = label == val\n",
    "\n",
    "        I[index] = float(np.sum(np.logical_and(label_i, pred_i)))\n",
    "        U[index] = float(np.sum(np.logical_or(label_i, pred_i)))\n",
    "\n",
    "    mean_iou = np.mean(I / U)\n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4857142857142857\n"
     ]
    }
   ],
   "source": [
    "print(compute_mean_iou(pred, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 evaluate_segmentation(pred, label, num_classes, score_averaging=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_segmentation(pred, label, num_classes, score_averaging=\"weighted\"):\n",
    "    flat_pred = pred.flatten()\n",
    "    flat_label = label.flatten()\n",
    "\n",
    "    global_accuracy = compute_global_accuracy(flat_pred, flat_label)\n",
    "    class_accuracies = compute_class_accuracies(flat_pred, flat_label, num_classes)\n",
    "\n",
    "    prec = precision_score(flat_pred, flat_label, average=score_averaging)\n",
    "    rec = recall_score(flat_pred, flat_label, average=score_averaging)\n",
    "    f1 = f1_score(flat_pred, flat_label, average=score_averaging)\n",
    "\n",
    "    iou = compute_mean_iou(flat_pred, flat_label)\n",
    "\n",
    "    return global_accuracy, class_accuracies, prec, rec, f1, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666,\n",
       " [0.5, 0.8],\n",
       " 0.7000000000000001,\n",
       " 0.6666666666666666,\n",
       " 0.6753246753246753,\n",
       " 0.4857142857142857)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_segmentation(pred, label, 2, score_averaging=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 compute_class_weights(labels_dir, label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(labels_dir, label_values):\n",
    "    '''\n",
    "    Arguments:\n",
    "        labels_dir(list): Directory where the image segmentation labels are\n",
    "        num_classes(int): the number of classes of pixels in all images\n",
    "\n",
    "    Returns:\n",
    "        class_weights(list): a list of class weights where each index represents each class label and the element is the class weight for that label.\n",
    "\n",
    "    '''\n",
    "    image_files = [os.path.join(labels_dir, file) for file in os.listdir(labels_dir) if file.endswith('.png')]\n",
    "\n",
    "    num_classes = len(label_values)\n",
    "\n",
    "    class_pixels = np.zeros(num_classes) \n",
    "\n",
    "    total_pixels = 0.0\n",
    "\n",
    "    for n in range(len(image_files)):\n",
    "        image = imread(image_files[n])\n",
    "\n",
    "        for index, colour in enumerate(label_values):\n",
    "            class_map = np.all(np.equal(image, colour), axis = -1)\n",
    "            class_map = class_map.astype(np.float32)\n",
    "            class_pixels[index] += np.sum(class_map)\n",
    "\n",
    "            \n",
    "        print(\"\\rProcessing image: \" + str(n) + \" / \" + str(len(image_files)), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    total_pixels = float(np.sum(class_pixels))\n",
    "    index_to_delete = np.argwhere(class_pixels==0.0)\n",
    "    class_pixels = np.delete(class_pixels, index_to_delete)\n",
    "\n",
    "    class_weights = total_pixels / class_pixels\n",
    "    class_weights = class_weights / np.sum(class_weights)\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the memory usage, for debugging\n",
    "def memory():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0]/2.**30  # Memory use in GB\n",
    "    print('Memory usage in GBs:', memoryUse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12148\n"
     ]
    }
   ],
   "source": [
    "pid = os.getpid()\n",
    "print(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psutil.Process(pid=12148, name='python.exe', started='21:00:46')\n"
     ]
    }
   ],
   "source": [
    "py = psutil.Process(pid)\n",
    "print(py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmem(rss=208134144, vms=184291328, num_page_faults=59173, peak_wset=208134144, wset=208134144, peak_paged_pool=549824, paged_pool=549552, peak_nonpaged_pool=1161536, nonpaged_pool=156408, pagefile=184291328, peak_pagefile=184291328, private=184291328)\n"
     ]
    }
   ],
   "source": [
    "print(py.memory_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19385147094726562\n"
     ]
    }
   ],
   "source": [
    "print(py.memory_info()[0]/2.**30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage in GBs: 0.19385528564453125\n"
     ]
    }
   ],
   "source": [
    "memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check all functions in main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os,time,cv2, sys, math\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import argparse\n",
    "import random\n",
    "import os, sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# import helpers \n",
    "# import utils \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"models\")\n",
    "from FC_DenseNet_Tiramisu import build_fc_densenet\n",
    "from Encoder_Decoder import build_encoder_decoder\n",
    "from RefineNet import build_refinenet\n",
    "from FRRN import build_frrn\n",
    "from MobileUNet import build_mobile_unet\n",
    "from PSPNet import build_pspnet\n",
    "from GCN import build_gcn\n",
    "from DeepLabV3 import build_deeplabv3\n",
    "from DeepLabV3_plus import build_deeplabv3_plus\n",
    "from AdapNet import build_adaptnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 str2bool(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    },
    {
     "ename": "ArgumentTypeError",
     "evalue": "Boolean value expected.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArgumentTypeError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-3d65186d81b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr2bool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr2bool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Yes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr2bool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Fool'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-4f5886fa087b>\u001b[0m in \u001b[0;36mstr2bool\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArgumentTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Boolean value expected.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mArgumentTypeError\u001b[0m: Boolean value expected."
     ]
    }
   ],
   "source": [
    "print(str2bool('yes'))\n",
    "print(str2bool('no'))\n",
    "print(str2bool('Yes'))\n",
    "print(str2bool('Fool'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--model'], dest='model', nargs=None, const=None, default='MobileUNet-Skip', type=<class 'str'>, choices=None, help='The model you are using. Currently supports:    FC-DenseNet56, FC-DenseNet67, FC-DenseNet103, Encoder-Decoder, Encoder-Decoder-Skip, RefineNet-Res50, RefineNet-Res101, RefineNet-Res152,     FRRN-A, FRRN-B, MobileUNet, MobileUNet-Skip, PSPNet-Res50, PSPNet-Res101, PSPNet-Res152, GCN-Res50, GCN-Res101, GCN-Res152, DeepLabV3-Res50     DeepLabV3-Res101, DeepLabV3-Res152, DeepLabV3_plus-Res50, DeepLabV3_plus-Res101, DeepLabV3_plus-Res152, AdapNet, custom', metavar=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num_epochs', type=int, default=1, help='Number of epochs to train for')\n",
    "parser.add_argument('--mode', type=str, default=\"train\", help='Select \"train\", \"test\", or \"predict\" mode. \\\n",
    "    Note that for prediction mode you have to specify an image to run the model on.')\n",
    "parser.add_argument('--class_balancing', type=str2bool, default=False, help='Whether to use median frequency class weights to balance the classes in the loss')\n",
    "parser.add_argument('--image', type=str, default=None, help='The image you want to predict on. Only valid in \"predict\" mode.')\n",
    "parser.add_argument('--continue_training', type=str2bool, default=False, help='Whether to continue training from a checkpoint')\n",
    "parser.add_argument('--dataset', type=str, default=\"CamVid\", help='Dataset you are using.')\n",
    "parser.add_argument('--crop_height', type=int, default=256, help='Height of cropped input image to network')\n",
    "parser.add_argument('--crop_width', type=int, default=256, help='Width of cropped input image to network')\n",
    "parser.add_argument('--batch_size', type=int, default=1, help='Number of images in each batch')\n",
    "parser.add_argument('--num_val_images', type=int, default=10, help='The number of images to used for validations')\n",
    "parser.add_argument('--h_flip', type=str2bool, default=False, help='Whether to randomly flip the image horizontally for data augmentation')\n",
    "parser.add_argument('--v_flip', type=str2bool, default=False, help='Whether to randomly flip the image vertically for data augmentation')\n",
    "parser.add_argument('--brightness', type=float, default=None, help='Whether to randomly change the image brightness for data augmentation. Specifies the max bightness change.')\n",
    "parser.add_argument('--rotation', type=float, default=None, help='Whether to randomly rotate the image for data augmentation. Specifies the max rotation angle.')\n",
    "parser.add_argument('--model', type=str, default=\"MobileUNet-Skip\", help='The model you are using. Currently supports:\\\n",
    "    FC-DenseNet56, FC-DenseNet67, FC-DenseNet103, Encoder-Decoder, Encoder-Decoder-Skip, RefineNet-Res50, RefineNet-Res101, RefineNet-Res152, \\\n",
    "    FRRN-A, FRRN-B, MobileUNet, MobileUNet-Skip, PSPNet-Res50, PSPNet-Res101, PSPNet-Res152, GCN-Res50, GCN-Res101, GCN-Res152, DeepLabV3-Res50 \\\n",
    "    DeepLabV3-Res101, DeepLabV3-Res152, DeepLabV3_plus-Res50, DeepLabV3_plus-Res101, DeepLabV3_plus-Res152, AdapNet, custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, brightness=None, class_balancing=False, continue_training=False, crop_height=256, crop_width=256, dataset='CamVid', h_flip=False, image=None, mode='train', model='MobileUNet-Skip', num_epochs=1, num_val_images=10, rotation=None, v_flip=False)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.crop_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 prepare_data(dataset_dir=args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the training, validation, and testing file paths\n",
    "def prepare_data(dataset_dir=args.dataset):\n",
    "    train_input_names=[]\n",
    "    train_output_names=[]\n",
    "    val_input_names=[]\n",
    "    val_output_names=[]\n",
    "    test_input_names=[]\n",
    "    test_output_names=[]\n",
    "    for file in os.listdir(dataset_dir + \"/train\"):\n",
    "        cwd = os.getcwd()\n",
    "        train_input_names.append(cwd + \"/\" + dataset_dir + \"/train/\" + file)\n",
    "    for file in os.listdir(dataset_dir + \"/train_labels\"):\n",
    "        cwd = os.getcwd()\n",
    "        train_output_names.append(cwd + \"/\" + dataset_dir + \"/train_labels/\" + file)\n",
    "    for file in os.listdir(dataset_dir + \"/val\"):\n",
    "        cwd = os.getcwd()\n",
    "        val_input_names.append(cwd + \"/\" + dataset_dir + \"/val/\" + file)\n",
    "    for file in os.listdir(dataset_dir + \"/val_labels\"):\n",
    "        cwd = os.getcwd()\n",
    "        val_output_names.append(cwd + \"/\" + dataset_dir + \"/val_labels/\" + file)\n",
    "    for file in os.listdir(dataset_dir + \"/test\"):\n",
    "        cwd = os.getcwd()\n",
    "        test_input_names.append(cwd + \"/\" + dataset_dir + \"/test/\" + file)\n",
    "    for file in os.listdir(dataset_dir + \"/test_labels\"):\n",
    "        cwd = os.getcwd()\n",
    "        test_output_names.append(cwd + \"/\" + dataset_dir + \"/test_labels/\" + file)\n",
    "    train_input_names.sort(),train_output_names.sort(), val_input_names.sort(), val_output_names.sort(), test_input_names.sort(), test_output_names.sort()\n",
    "    return train_input_names,train_output_names, val_input_names, val_output_names, test_input_names, test_output_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to prepare the paths of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'CamVid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0001TP_006690.png',\n",
       " '0001TP_006720.png',\n",
       " '0001TP_006750.png',\n",
       " '0001TP_006780.png',\n",
       " '0001TP_006810.png']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataset_dir + \"/train\")[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Python Workspace\\\\Semantic-Segmentation-Suite'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Python Workspace\\\\Semantic-Segmentation-Suite/CamVid/train/0001TP_006690.png'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() + \"/\" + dataset_dir + \"/train/\" + os.listdir(dataset_dir + \"/train\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above notation is compatible with linux platform but not with Windows platform!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_names,train_output_names, val_input_names, val_output_names, test_input_names, test_output_names = prepare_data(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Python Workspace\\\\Semantic-Segmentation-Suite/CamVid/val/0001TP_006870.png',\n",
       " 'E:\\\\Python Workspace\\\\Semantic-Segmentation-Suite/CamVid/val/0001TP_006900.png',\n",
       " 'E:\\\\Python Workspace\\\\Semantic-Segmentation-Suite/CamVid/val/0001TP_006930.png',\n",
       " 'E:\\\\Python Workspace\\\\Semantic-Segmentation-Suite/CamVid/val/0001TP_006960.png',\n",
       " 'E:\\\\Python Workspace\\\\Semantic-Segmentation-Suite/CamVid/val/0001TP_007530.png']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_input_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 load_image(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    image = cv2.cvtColor(cv2.imread(path,-1), cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('test',load_image(val_input_names[0]))\n",
    "cv2.waitKey(-1)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 960, 3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_image(val_input_names[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_image(val_input_names[0]).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 data_augmentation(input_image, output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(input_image, output_image):\n",
    "    # Data augmentation\n",
    "    input_image, output_image = utils.random_crop(input_image, output_image, args.crop_height, args.crop_width)\n",
    "\n",
    "    if args.h_flip and random.randint(0,1):\n",
    "        input_image = cv2.flip(input_image, 1)\n",
    "        output_image = cv2.flip(output_image, 1)\n",
    "    if args.v_flip and random.randint(0,1):\n",
    "        input_image = cv2.flip(input_image, 0)\n",
    "        output_image = cv2.flip(output_image, 0)\n",
    "    if args.brightness:\n",
    "        factor = random.uniform(-1*args.brightness, args.brightness)\n",
    "        table = np.array([((i / 255.0) ** factor) * 255 for i in np.arange(0, 256)]).astype(np.uint8)\n",
    "        input_image = cv2.LUT(input_image, table)\n",
    "    if args.rotation:\n",
    "        angle = random.uniform(-1*args.rotation, args.rotation)\n",
    "    if args.rotation:\n",
    "        M = cv2.getRotationMatrix2D((input_image.shape[1]//2, input_image.shape[0]//2), angle, 1.0)\n",
    "        input_image = cv2.warpAffine(input_image, M, (input_image.shape[1], input_image.shape[0]), flags=INTER_NEAREST)\n",
    "        output_image = cv2.warpAffine(output_image, M, (output_image.shape[1], output_image.shape[0]), flags=INTER_NEAREST)\n",
    "\n",
    "    return input_image, output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 960, 3)\n",
      "(720, 960, 3)\n"
     ]
    }
   ],
   "source": [
    "input_raw = load_image(val_input_names[0])\n",
    "output_raw = load_image(val_output_names[0])\n",
    "print(input_raw.shape)\n",
    "print(output_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "input_cropped, output_cropped = random_crop(input_raw, output_raw, args.crop_height, args.crop_width)\n",
    "print(input_cropped.shape)\n",
    "print(output_cropped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.h_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "args.brightness = 0.5\n",
    "print(args.brightness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15869883118469497"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.uniform(-1*args.brightness, args.brightness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  15,  22,  27,  31,  35,  39,  42,  45,  47,  50,  52,  55,\n",
       "        57,  59,  61,  63,  65,  67,  69,  71,  73,  74,  76,  78,  79,\n",
       "        81,  82,  84,  85,  87,  88,  90,  91,  93,  94,  95,  97,  98,\n",
       "        99, 100, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 114,\n",
       "       115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127,\n",
       "       128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
       "       141, 141, 142, 143, 144, 145, 146, 147, 148, 148, 149, 150, 151,\n",
       "       152, 153, 153, 154, 155, 156, 157, 158, 158, 159, 160, 161, 162,\n",
       "       162, 163, 164, 165, 165, 166, 167, 168, 168, 169, 170, 171, 171,\n",
       "       172, 173, 174, 174, 175, 176, 177, 177, 178, 179, 179, 180, 181,\n",
       "       182, 182, 183, 184, 184, 185, 186, 186, 187, 188, 188, 189, 190,\n",
       "       190, 191, 192, 192, 193, 194, 194, 195, 196, 196, 197, 198, 198,\n",
       "       199, 200, 200, 201, 201, 202, 203, 203, 204, 205, 205, 206, 206,\n",
       "       207, 208, 208, 209, 210, 210, 211, 211, 212, 213, 213, 214, 214,\n",
       "       215, 216, 216, 217, 217, 218, 218, 219, 220, 220, 221, 221, 222,\n",
       "       222, 223, 224, 224, 225, 225, 226, 226, 227, 228, 228, 229, 229,\n",
       "       230, 230, 231, 231, 232, 233, 233, 234, 234, 235, 235, 236, 236,\n",
       "       237, 237, 238, 238, 239, 240, 240, 241, 241, 242, 242, 243, 243,\n",
       "       244, 244, 245, 245, 246, 246, 247, 247, 248, 248, 249, 249, 250,\n",
       "       250, 251, 251, 252, 252, 253, 253, 254, 255], dtype=uint8)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([((i / 255.0) ** 0.5) * 255 for i in np.arange(0, 256)]).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865476"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_LUT = cv2.LUT(input_cropped, np.array([((i / 255.0) ** 0.5) * 255 for i in np.arange(0, 256)]).astype(np.uint8))\n",
    "output_LUT = cv2.LUT(output_cropped, np.array([((i / 255.0) ** 0.5) * 255 for i in np.arange(0, 256)]).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('input_cropped',input_cropped)\n",
    "cv2.imshow('input_LUT',input_LUT)\n",
    "cv2.waitKey(-1)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above test indicate that the definition of \"factor = random.uniform(-1*args.brightness, args.brightness)\" is problematic. We have to set \"factor\" in the range of \\[0,+infinite\\]. So the function is modified as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(input_image, output_image):\n",
    "    # Data augmentation\n",
    "    input_image, output_image = utils.random_crop(input_image, output_image, args.crop_height, args.crop_width)\n",
    "\n",
    "    if args.h_flip and random.randint(0,1):\n",
    "        input_image = cv2.flip(input_image, 1)\n",
    "        output_image = cv2.flip(output_image, 1)\n",
    "    if args.v_flip and random.randint(0,1):\n",
    "        input_image = cv2.flip(input_image, 0)\n",
    "        output_image = cv2.flip(output_image, 0)\n",
    "    if args.brightness:\n",
    "        factor = random.uniform(np.exp(-1*args.brightness), np.exp(args.brightness))\n",
    "        table = np.array([((i / 255.0) ** factor) * 255 for i in np.arange(0, 256)]).astype(np.uint8)\n",
    "        input_image = cv2.LUT(input_image, table)\n",
    "    if args.rotation:\n",
    "        angle = random.uniform(-1*args.rotation, args.rotation)\n",
    "        M = cv2.getRotationMatrix2D((input_image.shape[1]//2, input_image.shape[0]//2), angle, 1.0)\n",
    "        input_image = cv2.warpAffine(input_image, M, (input_image.shape[1], input_image.shape[0]), flags=INTER_NEAREST)\n",
    "        output_image = cv2.warpAffine(output_image, M, (output_image.shape[1], output_image.shape[0]), flags=INTER_NEAREST)\n",
    "\n",
    "    return input_image, output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6487212707001282"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6065306597126334"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.271366606701244"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.uniform(np.exp(-1*args.brightness), np.exp(args.brightness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "args.rotation = 45\n",
    "print(args.rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.74625954458827"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.uniform(-1*args.rotation, args.rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.70710678,   0.70710678, -53.01933598],\n",
       "       [ -0.70710678,   0.70710678, 128.        ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.getRotationMatrix2D((input_LUT.shape[1]//2, input_LUT.shape[0]//2), 45, 1.0) # rotation center.x;center.y;degree;scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8775825618903728"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cos(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_LUT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'INTER_NEAREST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-30c508d9fdb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m input_rotated = cv2.warpAffine(input_LUT, \n\u001b[0;32m      2\u001b[0m                                \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetRotationMatrix2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_LUT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_LUT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m45\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                                (input_LUT.shape[1], input_LUT.shape[0]), flags=INTER_NEAREST)\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'INTER_NEAREST' is not defined"
     ]
    }
   ],
   "source": [
    "input_rotated = cv2.warpAffine(input_LUT, \n",
    "                               cv2.getRotationMatrix2D((input_LUT.shape[1]//2, input_LUT.shape[0]//2), 45, 1.0), \n",
    "                               (input_LUT.shape[1], input_LUT.shape[0]), flags=INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See, this code has error. I have to modify this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rotated = cv2.warpAffine(input_LUT, \n",
    "                               cv2.getRotationMatrix2D((input_LUT.shape[1]//2, input_LUT.shape[0]//2), 45, 1.0), \n",
    "                               (input_LUT.shape[1], input_LUT.shape[0]), flags=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('input_LUT',input_LUT)\n",
    "cv2.imshow('input_rotated',input_rotated)\n",
    "cv2.waitKey(-1)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final version of the function should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(input_image, output_image):\n",
    "    # Data augmentation\n",
    "    input_image, output_image = random_crop(input_image, output_image, args.crop_height, args.crop_width)\n",
    "\n",
    "    if args.h_flip and random.randint(0,1):\n",
    "        input_image = cv2.flip(input_image, 1)\n",
    "        output_image = cv2.flip(output_image, 1)\n",
    "    if args.v_flip and random.randint(0,1):\n",
    "        input_image = cv2.flip(input_image, 0)\n",
    "        output_image = cv2.flip(output_image, 0)\n",
    "    if args.brightness:\n",
    "        factor = random.uniform(np.exp(-1*args.brightness), np.exp(args.brightness))\n",
    "        table = np.array([((i / 255.0) ** factor) * 255 for i in np.arange(0, 256)]).astype(np.uint8)\n",
    "        input_image = cv2.LUT(input_image, table)\n",
    "    if args.rotation:\n",
    "        angle = random.uniform(-1*args.rotation, args.rotation)\n",
    "        M = cv2.getRotationMatrix2D((input_image.shape[1]//2, input_image.shape[0]//2), angle, 1.0)\n",
    "        input_image = cv2.warpAffine(input_image, M, (input_image.shape[1], input_image.shape[0]), flags=cv2.INTER_NEAREST)\n",
    "        output_image = cv2.warpAffine(output_image, M, (output_image.shape[1], output_image.shape[0]), flags=cv2.INTER_NEAREST)\n",
    "\n",
    "    return input_image, output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 download_checkpoints(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_checkpoints(model_name):\n",
    "    subprocess.check_output([\"python\", \"get_pretrained_checkpoints.py\", \"--model=\" + model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  E  Program and Projects\\n  0754-B8FD\\n\\n E:\\\\Python Workspace\\\\Semantic-Segmentation-Suite \\n\\n2018/05/04  21:11    <DIR>          .\\n2018/05/04  21:11    <DIR>          ..\\n2018/04/17  17:04               160 .gitignore\\n2018/04/20  14:20    <DIR>          .ipynb_checkpoints\\n2018/04/17  17:04    <DIR>          CamVid\\n2018/04/20  14:01    <DIR>          checkpoints\\n2018/04/17  17:04             4,951 get_pretrained_checkpoints.py\\n2018/04/20  11:42             4,318 helpers.py\\n2018/04/17  17:04    <DIR>          Images\\n2018/05/04  21:11            82,087 Learn_the_code.ipynb\\n2018/04/20  14:51            24,447 main.py\\n2018/04/27  09:16    <DIR>          models\\n2018/04/17  17:04            12,358 README.md\\n2018/04/17  17:04    <DIR>          Test\\n2018/04/17  17:04             6,443 utils.py\\n2018/04/20  11:42    <DIR>          __pycache__\\n               7         134,764 \\n               9  238,425,235,456 \\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.check_output(\"dir\", shell=True, universal_newlines=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  E  Program and Projects\\n  0754-B8FD\\n\\n E:\\\\Python Workspace\\\\Semantic-Segmentation-Suite \\n\\n2018/05/04  21:11    <DIR>          .\\n2018/05/04  21:11    <DIR>          ..\\n2018/04/17  17:04               160 .gitignore\\n2018/04/20  14:20    <DIR>          .ipynb_checkpoints\\n2018/04/17  17:04    <DIR>          CamVid\\n2018/04/20  14:01    <DIR>          checkpoints\\n2018/04/17  17:04             4,951 get_pretrained_checkpoints.py\\n2018/04/20  11:42             4,318 helpers.py\\n2018/04/17  17:04    <DIR>          Images\\n2018/05/04  21:11            82,087 Learn_the_code.ipynb\\n2018/04/20  14:51            24,447 main.py\\n2018/04/27  09:16    <DIR>          models\\n2018/04/17  17:04            12,358 README.md\\n2018/04/17  17:04    <DIR>          Test\\n2018/04/17  17:04             6,443 utils.py\\n2018/04/20  11:42    <DIR>          __pycache__\\n               7         134,764 \\n               9  238,425,235,456 \\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(\"dir\", shell=True, universal_newlines=True, check=True, stdout=subprocess.PIPE).stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['python', 'get_pretrained_checkpoints.py', '--model=Res101']' returned non-zero exit status 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-df9b98bcae15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdownload_checkpoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Res101\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-91-1e94523a3772>\u001b[0m in \u001b[0;36mdownload_checkpoints\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdownload_checkpoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"python\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"get_pretrained_checkpoints.py\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"--model=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[1;32m--> 316\u001b[1;33m                **kwargs).stdout\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[1;32m--> 398\u001b[1;33m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['python', 'get_pretrained_checkpoints.py', '--model=Res101']' returned non-zero exit status 1"
     ]
    }
   ],
   "source": [
    "download_checkpoints(\"Res101\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Windows, one has to download and extract the file by oneself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 get_label_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the classes so we can record the evaluation results\n",
    "class_names_list, label_values = get_label_info(os.path.join(args.dataset, \"class_dict.csv\"))\n",
    "class_names_string = \"\"\n",
    "for class_name in class_names_list:\n",
    "    if not class_name == class_names_list[-1]:\n",
    "        class_names_string = class_names_string + class_name + \", \"\n",
    "    else:\n",
    "        class_names_string = class_names_string + class_name\n",
    "\n",
    "num_classes = len(label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CamVid'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal, Archway, Bicyclist, Bridge, Building, Car, CartLuggagePram, Child, Column_Pole, Fence, LaneMkgsDriv, LaneMkgsNonDriv, Misc_Text, MotorcycleScooter, OtherMoving, ParkingBlock, Pedestrian, Road, RoadShoulder, Sidewalk, SignSymbol, Sky, SUVPickupTruck, TrafficCone, TrafficLight, Train, Tree, Truck_Bus, Tunnel, VegetationMisc, Void, Wall\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print(class_names_string)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess=tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the model ...\n"
     ]
    }
   ],
   "source": [
    "# Get the selected model. \n",
    "# Some of they require pre-trained ResNet\n",
    "print(\"Preparing the model ...\")\n",
    "input = tf.placeholder(tf.float32,shape=[None,None,None,3])\n",
    "output = tf.placeholder(tf.float32,shape=[None,None,None,num_classes]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.1 PSPNet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "import numpy as np\n",
    "import resnet_v2\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Upsampling(inputs,feature_map_shape):\n",
    "    return tf.image.resize_bilinear(inputs, size=feature_map_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvUpscaleBlock(inputs, n_filters, kernel_size=[3, 3], scale=2):\n",
    "    \"\"\"\n",
    "    Basic conv transpose block for Encoder-Decoder upsampling\n",
    "    Apply successivly Transposed Convolution, BatchNormalization, ReLU nonlinearity\n",
    "    \"\"\"\n",
    "    net = slim.conv2d_transpose(inputs, n_filters, kernel_size=[3, 3], stride=[2, 2], activation_fn=None)\n",
    "    net = tf.nn.relu(slim.batch_norm(net, fused=True))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(inputs, n_filters, kernel_size=[3, 3]):\n",
    "    \"\"\"\n",
    "    Basic conv block for Encoder-Decoder\n",
    "    Apply successivly Convolution, BatchNormalization, ReLU nonlinearity\n",
    "    \"\"\"\n",
    "    net = slim.conv2d(inputs, n_filters, kernel_size, activation_fn=None, normalizer_fn=None)\n",
    "    net = tf.nn.relu(slim.batch_norm(net, fused=True))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InterpBlock(net, level, feature_map_shape, pooling_type):\n",
    "    \n",
    "    # Compute the kernel and stride sizes according to how large the final feature map will be\n",
    "    # When the kernel size and strides are equal, then we can compute the final feature map size\n",
    "    # by simply dividing the current size by the kernel or stride size\n",
    "    # The final feature map sizes are 1x1, 2x2, 3x3, and 6x6. We round to the closest integer\n",
    "    kernel_size = [int(np.round(float(feature_map_shape[0]) / float(level))), int(np.round(float(feature_map_shape[1]) / float(level)))]\n",
    "    stride_size = kernel_size\n",
    "\n",
    "    net = slim.pool(net, kernel_size, stride=stride_size, pooling_type='MAX')\n",
    "    net = slim.conv2d(net, 512, [1, 1], activation_fn=None)\n",
    "    net = slim.batch_norm(net, fused=True)\n",
    "    net = tf.nn.relu(net)\n",
    "    net = Upsampling(net, feature_map_shape)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PyramidPoolingModule(inputs, feature_map_shape, pooling_type):\n",
    "    \"\"\"\n",
    "    Build the Pyramid Pooling Module.\n",
    "    \"\"\"\n",
    "\n",
    "    interp_block1 = InterpBlock(inputs, 1, feature_map_shape, pooling_type)\n",
    "    interp_block2 = InterpBlock(inputs, 2, feature_map_shape, pooling_type)\n",
    "    interp_block3 = InterpBlock(inputs, 3, feature_map_shape, pooling_type)\n",
    "    interp_block6 = InterpBlock(inputs, 6, feature_map_shape, pooling_type)\n",
    "\n",
    "    res = tf.concat([inputs, interp_block6, interp_block3, interp_block2, interp_block1], axis=-1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pspnet(inputs, label_size, num_classes, preset_model='PSPNet-Res50', pooling_type = \"MAX\",\n",
    "    weight_decay=1e-5, upscaling_method=\"conv\", is_training=True, pretrained_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Builds the PSPNet model. \n",
    "\n",
    "    Arguments:\n",
    "      inputs: The input tensor\n",
    "      label_size: Size of the final label tensor. We need to know this for proper upscaling \n",
    "      preset_model: Which model you want to use. Select which ResNet model to use for feature extraction \n",
    "      num_classes: Number of classes\n",
    "      pooling_type: Max or Average pooling\n",
    "\n",
    "    Returns:\n",
    "      PSPNet model\n",
    "    \"\"\"\n",
    "\n",
    "    if preset_model == 'PSPNet-Res50':\n",
    "        with slim.arg_scope(resnet_v2.resnet_arg_scope(weight_decay=weight_decay)):\n",
    "            logits, end_points = resnet_v2.resnet_v2_50(inputs, is_training=is_training, scope='resnet_v2_50')\n",
    "            resnet_scope='resnet_v2_50'\n",
    "            # PSPNet requires pre-trained ResNet weights\n",
    "            init_fn = slim.assign_from_checkpoint_fn(os.path.join(pretrained_dir, 'resnet_v2_50.ckpt'), slim.get_model_variables('resnet_v2_50'))\n",
    "    elif preset_model == 'PSPNet-Res101':\n",
    "        with slim.arg_scope(resnet_v2.resnet_arg_scope(weight_decay=weight_decay)):\n",
    "            logits, end_points = resnet_v2.resnet_v2_101(inputs, is_training=is_training, scope='resnet_v2_101')\n",
    "            resnet_scope='resnet_v2_101'\n",
    "            # PSPNet requires pre-trained ResNet weights\n",
    "            init_fn = slim.assign_from_checkpoint_fn(os.path.join(pretrained_dir, 'resnet_v2_101.ckpt'), slim.get_model_variables('resnet_v2_101'))\n",
    "    elif preset_model == 'PSPNet-Res152':\n",
    "        with slim.arg_scope(resnet_v2.resnet_arg_scope(weight_decay=weight_decay)):\n",
    "            logits, end_points = resnet_v2.resnet_v2_152(inputs, is_training=is_training, scope='resnet_v2_152')\n",
    "            resnet_scope='resnet_v2_152'\n",
    "            # PSPNet requires pre-trained ResNet weights\n",
    "            init_fn = slim.assign_from_checkpoint_fn(os.path.join(pretrained_dir, 'resnet_v2_152.ckpt'), slim.get_model_variables('resnet_v2_152'))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported ResNet model '%s'. This function only supports ResNet 50, ResNet 101, and ResNet 152\" % (preset_model))\n",
    "\n",
    "    feature_map_shape = [int(x / 8.0) for x in label_size]\n",
    "    print(feature_map_shape)\n",
    "    psp = PyramidPoolingModule(end_points['pool3'], feature_map_shape=feature_map_shape, pooling_type=pooling_type)\n",
    "\n",
    "    net = slim.conv2d(psp, 512, [3, 3], activation_fn=None)\n",
    "    net = slim.batch_norm(net, fused=True)\n",
    "    net = tf.nn.relu(net)\n",
    "\n",
    "    if upscaling_method.lower() == \"conv\":\n",
    "        net = ConvUpscaleBlock(net, 256, kernel_size=[3, 3], scale=2)\n",
    "        net = ConvBlock(net, 256)\n",
    "        net = ConvUpscaleBlock(net, 128, kernel_size=[3, 3], scale=2)\n",
    "        net = ConvBlock(net, 128)\n",
    "        net = ConvUpscaleBlock(net, 64, kernel_size=[3, 3], scale=2)\n",
    "        net = ConvBlock(net, 64)\n",
    "    elif upscaling_method.lower() == \"bilinear\":\n",
    "        net = Upsampling(net, label_size)\n",
    "    \n",
    "    net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope='logits')\n",
    "\n",
    "    return net, init_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Redundent Redefinition\n",
    "def mean_image_subtraction(inputs, means=[123.68, 116.78, 103.94]):\n",
    "    inputs=tf.to_float(inputs)\n",
    "    num_channels = inputs.get_shape().as_list()[-1]\n",
    "    if len(means) != num_channels:\n",
    "        raise ValueError('len(means) must match the number of channels')\n",
    "    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=inputs)\n",
    "    for i in range(num_channels):\n",
    "        channels[i] -= means[i]\n",
    "    return tf.concat(axis=3, values=channels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.2 build_pspnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model = \"PSPNet-Res50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 32]\n"
     ]
    }
   ],
   "source": [
    "    # Image size is required for PSPNet\n",
    "    # PSPNet requires pre-trained ResNet weights\n",
    "    network, init_fn = build_pspnet(input, label_size=[args.crop_height, args.crop_width], preset_model = args.model, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "256\n",
      "PSPNet-Res50\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "print(args.crop_height)\n",
    "print(args.crop_width)\n",
    "print(args.model)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"logits/BiasAdd:0\", shape=(?, 256, 256, 32), dtype=float32)\n",
      "<function assign_from_checkpoint_fn.<locals>.callback at 0x000001B9C84E0D90>\n"
     ]
    }
   ],
   "source": [
    "print(network)\n",
    "print(init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.3 Compute your softmax cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute your softmax cross entropy loss\n",
    "loss = None\n",
    "if args.class_balancing:\n",
    "    print(\"Computing class weights for\", args.dataset, \"...\")\n",
    "    class_weights = utils.compute_class_weights(labels_dir=args.dataset + \"/train_labels\", label_values=label_values)\n",
    "    unweighted_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=network, labels=output))\n",
    "    loss = tf.reduce_mean(unweighted_loss * class_weights)\n",
    "else:\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=network, labels=output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(args.class_balancing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.4 others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(0.0001).minimize(loss, var_list=[var for var in tf.trainable_variables()])\n",
    "\n",
    "saver=tf.train.Saver(max_to_keep=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 initialize vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 36991904 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models\\resnet_v2_50.ckpt\n"
     ]
    }
   ],
   "source": [
    "# If a pre-trained ResNet is required, load the weights.\n",
    "# This must be done AFTER the variables are initialized with sess.run(tf.global_variables_initializer())\n",
    "if init_fn is not None:\n",
    "    init_fn(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    }
   ],
   "source": [
    "print(args.mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Begin training *****\n",
      "Dataset --> CamVid\n",
      "Model --> PSPNet-Res50\n",
      "Crop Height --> 256\n",
      "Crop Width --> 256\n",
      "Num Epochs --> 1\n",
      "Batch Size --> 1\n",
      "Num Classes --> 32\n",
      "Data Augmentation:\n",
      "\tVertical Flip --> False\n",
      "\tHorizontal Flip --> False\n",
      "\tBrightness Alteration --> 0.5\n",
      "\tRotation --> 45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(\"\\n***** Begin training *****\")\n",
    "    print(\"Dataset -->\", args.dataset)\n",
    "    print(\"Model -->\", args.model)\n",
    "    print(\"Crop Height -->\", args.crop_height)\n",
    "    print(\"Crop Width -->\", args.crop_width)\n",
    "    print(\"Num Epochs -->\", args.num_epochs)\n",
    "    print(\"Batch Size -->\", args.batch_size)\n",
    "    print(\"Num Classes -->\", num_classes)\n",
    "\n",
    "    print(\"Data Augmentation:\")\n",
    "    print(\"\\tVertical Flip -->\", args.v_flip)\n",
    "    print(\"\\tHorizontal Flip -->\", args.h_flip)\n",
    "    print(\"\\tBrightness Alteration -->\", args.brightness)\n",
    "    print(\"\\tRotation -->\", args.rotation)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.num_val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256, 256, 3)\n",
      "(1, 256, 256, 3, 32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 256, 256, 3, 32) for Tensor 'Placeholder_1:0', which has shape '(?, ?, ?, 32)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-8b0f6deedb01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m# Do the training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0minput_image_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0moutput_image_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mcurrent_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    973\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    976\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1, 256, 256, 3, 32) for Tensor 'Placeholder_1:0', which has shape '(?, ?, ?, 32)'"
     ]
    }
   ],
   "source": [
    "avg_loss_per_epoch = []\n",
    "\n",
    "# Which validation images do we want\n",
    "val_indices = []\n",
    "num_vals = min(args.num_val_images, len(val_input_names))\n",
    "\n",
    "# Set random seed to make sure models are validated on the same validation images.\n",
    "# So you can compare the results of different models more intuitively.\n",
    "random.seed(16)\n",
    "val_indices=random.sample(range(0,len(val_input_names)),num_vals)\n",
    "\n",
    "# Do the training here\n",
    "for epoch in range(0, args.num_epochs):\n",
    "\n",
    "    current_losses = []\n",
    "\n",
    "    cnt=0\n",
    "\n",
    "    # Equivalent to shuffling\n",
    "    id_list = np.random.permutation(len(train_input_names))\n",
    "\n",
    "    num_iters = int(np.floor(len(id_list) / args.batch_size))\n",
    "    st = time.time()\n",
    "    epoch_st=time.time()\n",
    "    for i in range(num_iters):\n",
    "        # st=time.time()\n",
    "\n",
    "        input_image_batch = []\n",
    "        output_image_batch = [] \n",
    "\n",
    "        # Collect a batch of images\n",
    "        for j in range(args.batch_size):\n",
    "            index = i*args.batch_size + j\n",
    "            id = id_list[index]\n",
    "            input_image = load_image(train_input_names[id])\n",
    "            output_image = load_image(train_output_names[id])\n",
    "\n",
    "            # with tf.device('/cpu:0'):\n",
    "            input_image, output_image = data_augmentation(input_image, output_image)\n",
    "\n",
    "\n",
    "            # Prep the data. Make sure the labels are in one-hot format\n",
    "            input_image = np.float32(input_image) / 255.0\n",
    "            output_image = np.float32(one_hot_it(label=output_image, label_values=label_values))\n",
    "\n",
    "            input_image_batch.append(np.expand_dims(input_image, axis=0))\n",
    "            output_image_batch.append(np.expand_dims(output_image, axis=0))\n",
    "                \n",
    "\n",
    "        # ***** THIS CAUSES A MEMORY LEAK AS NEW TENSORS KEEP GETTING CREATED *****\n",
    "        # input_image = tf.image.crop_to_bounding_box(input_image, offset_height=0, offset_width=0, \n",
    "        #                                               target_height=args.crop_height, target_width=args.crop_width).eval(session=sess)\n",
    "        # output_image = tf.image.crop_to_bounding_box(output_image, offset_height=0, offset_width=0, \n",
    "        #                                               target_height=args.crop_height, target_width=args.crop_width).eval(session=sess)\n",
    "        # ***** THIS CAUSES A MEMORY LEAK AS NEW TENSORS KEEP GETTING CREATED *****\n",
    "\n",
    "        # memory()\n",
    "\n",
    "        if args.batch_size == 1:\n",
    "            input_image_batch = input_image_batch[0]\n",
    "            print(input_image_batch.shape)\n",
    "            output_image_batch = output_image_batch[0]\n",
    "            print(output_image_batch.shape)\n",
    "        else:\n",
    "            input_image_batch = np.squeeze(np.stack(input_image_batch, axis=1))\n",
    "            output_image_batch = np.squeeze(np.stack(output_image_batch, axis=1))\n",
    "\n",
    "        # Do the training\n",
    "        _,current=sess.run([opt,loss],feed_dict={input:input_image_batch,output:output_image_batch})\n",
    "        current_losses.append(current)\n",
    "        cnt = cnt + args.batch_size\n",
    "        if cnt % 20 == 0:\n",
    "            string_print = \"Epoch = %d Count = %d Current_Loss = %.4f Time = %.2f\"%(epoch,cnt,current,time.time()-st)\n",
    "            LOG(string_print)\n",
    "            st = time.time()\n",
    "\n",
    "    mean_loss = np.mean(current_losses)\n",
    "    avg_loss_per_epoch.append(mean_loss)\n",
    "\n",
    "    # Create directories if needed\n",
    "    if not os.path.isdir(\"%s/%04d\"%(\"checkpoints\",epoch)):\n",
    "        os.makedirs(\"%s/%04d\"%(\"checkpoints\",epoch))\n",
    "\n",
    "    # The following code has problem since model_checkpoint_name is not defined!!!\n",
    "    # saver.save(sess,model_checkpoint_name)\n",
    "\n",
    "    if val_indices != 0:\n",
    "        saver.save(sess,\"%s/%04d/model.ckpt\"%(\"checkpoints\",epoch))\n",
    "\n",
    "\n",
    "    target=open(\"%s/%04d/val_scores.csv\"%(\"checkpoints\",epoch),'w')\n",
    "    target.write(\"val_name, avg_accuracy, precision, recall, f1 score, mean iou, %s\\n\" % (class_names_string))\n",
    "\n",
    "\n",
    "    scores_list = []\n",
    "    class_scores_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    iou_list = []\n",
    "\n",
    "\n",
    "    # Do the validation on a small set of validation images\n",
    "    for ind in val_indices:\n",
    "\n",
    "        input_image = np.expand_dims(np.float32(load_image(val_input_names[ind])[:args.crop_height, :args.crop_width]),axis=0)/255.0\n",
    "        gt = load_image(val_output_names[ind])[:args.crop_height, :args.crop_width]\n",
    "        gt = helpers.reverse_one_hot(helpers.one_hot_it(gt, label_values))\n",
    "\n",
    "        # st = time.time()\n",
    "\n",
    "        output_image = sess.run(network,feed_dict={input:input_image})\n",
    "\n",
    "\n",
    "        output_image = np.array(output_image[0,:,:,:])\n",
    "        output_image = helpers.reverse_one_hot(output_image)\n",
    "        out_vis_image = helpers.colour_code_segmentation(output_image, label_values)\n",
    "\n",
    "        accuracy, class_accuracies, prec, rec, f1, iou = utils.evaluate_segmentation(pred=output_image, label=gt, num_classes=num_classes)\n",
    "\n",
    "        file_name = utils.filepath_to_name(val_input_names[ind])\n",
    "        target.write(\"%s, %f, %f, %f, %f, %f\"%(file_name, accuracy, prec, rec, f1, iou))\n",
    "        for item in class_accuracies:\n",
    "            target.write(\", %f\"%(item))\n",
    "        target.write(\"\\n\")\n",
    "\n",
    "        scores_list.append(accuracy)\n",
    "        class_scores_list.append(class_accuracies)\n",
    "        precision_list.append(prec)\n",
    "        recall_list.append(rec)\n",
    "        f1_list.append(f1)\n",
    "        iou_list.append(iou)\n",
    "\n",
    "        gt = helpers.colour_code_segmentation(gt, label_values)\n",
    "\n",
    "        file_name = os.path.basename(val_input_names[ind])\n",
    "        file_name = os.path.splitext(file_name)[0]\n",
    "        cv2.imwrite(\"%s/%04d/%s_pred.png\"%(\"checkpoints\",epoch, file_name),cv2.cvtColor(np.uint8(out_vis_image), cv2.COLOR_RGB2BGR))\n",
    "        cv2.imwrite(\"%s/%04d/%s_gt.png\"%(\"checkpoints\",epoch, file_name),cv2.cvtColor(np.uint8(gt), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "    target.close()\n",
    "\n",
    "    avg_score = np.mean(scores_list)\n",
    "    class_avg_scores = np.mean(class_scores_list, axis=0)\n",
    "    avg_scores_per_epoch.append(avg_score)\n",
    "    avg_precision = np.mean(precision_list)\n",
    "    avg_recall = np.mean(recall_list)\n",
    "    avg_f1 = np.mean(f1_list)\n",
    "    avg_iou = np.mean(iou_list)\n",
    "\n",
    "    print(\"\\nAverage validation accuracy for epoch # %04d = %f\"% (epoch, avg_score))\n",
    "    print(\"Average per class validation accuracies for epoch # %04d:\"% (epoch))\n",
    "    for index, item in enumerate(class_avg_scores):\n",
    "        print(\"%s = %f\" % (class_names_list[index], item))\n",
    "    print(\"Validation precision = \", avg_precision)\n",
    "    print(\"Validation recall = \", avg_recall)\n",
    "    print(\"Validation F1 score = \", avg_f1)\n",
    "    print(\"Validation IoU score = \", avg_iou)\n",
    "\n",
    "    epoch_time=time.time()-epoch_st\n",
    "    remain_time=epoch_time*(args.num_epochs-1-epoch)\n",
    "    m, s = divmod(remain_time, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    if s!=0:\n",
    "        train_time=\"Remaining training time = %d hours %d minutes %d seconds\\n\"%(h,m,s)\n",
    "    else:\n",
    "        train_time=\"Remaining training time : Training completed.\\n\"\n",
    "    utils.LOG(train_time)\n",
    "    scores_list = []\n",
    "\n",
    "fig = plt.figure(figsize=(11,8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "ax1.plot(range(args.num_epochs), avg_scores_per_epoch)\n",
    "ax1.set_title(\"Average validation accuracy vs epochs\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Avg. val. accuracy\")\n",
    "\n",
    "\n",
    "plt.savefig('accuracy_vs_epochs.png')\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "ax1.plot(range(args.num_epochs), avg_loss_per_epoch)\n",
    "ax1.set_title(\"Average loss vs epochs\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Current loss\")\n",
    "\n",
    "plt.savefig('loss_vs_epochs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
